{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c00a8f-c9fb-4cac-baa2-a42030607205",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Bayes' theorem?\n",
    "Ans-\n",
    "   Bayes' theorem is a fundamental concept in probability theory that describes how to update the probability of a hypothesis (an event or a proposition) based on new evidence or observations. It is named after Thomas Bayes, an 18th-century English mathematician and Presbyterian minister who first introduced the concept.\n",
    "\n",
    "The theorem states that the probability of a hypothesis H given evidence E is proportional to the probability of the evidence E given the hypothesis H, multiplied by the prior probability of the hypothesis H, and divided by the probability of the evidence E. Mathematically, Bayes' theorem can be expressed as:\n",
    "\n",
    "P(H|E) = P(E|H) * P(H) / P(E)\n",
    "\n",
    "where P(H|E) is the probability of the hypothesis H given the evidence E, P(E|H) is the probability of the evidence E given the hypothesis H, P(H) is the prior probability of the hypothesis H, and P(E) is the probability of the evidence E.\n",
    "\n",
    "In simpler terms, Bayes' theorem tells us how to update our beliefs about the probability of a hypothesis based on new evidence. It involves combining our prior knowledge about the hypothesis with the new evidence to arrive at a more accurate estimate of the probability of the hypothesis.\n",
    "\n",
    "Bayes' theorem is widely used in various fields, including statistics, machine learning, and artificial intelligence, to model uncertain events and make predictions based on incomplete or noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8560d32d-8af9-4274-9e37-cbc54182da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the formula for Bayes' theorem?\n",
    "Ans-\n",
    "    The formula for Bayes' theorem is as follows:\n",
    "\n",
    "P(H|E) = P(E|H) * P(H) / P(E)\n",
    "\n",
    "where:\n",
    "\n",
    "P(H|E) is the probability of the hypothesis H given the evidence E, also known as the posterior probability.\n",
    "P(E|H) is the probability of the evidence E given the hypothesis H, also known as the likelihood.\n",
    "P(H) is the prior probability of the hypothesis H, which is the probability of the hypothesis before the evidence is taken into account.\n",
    "P(E) is the probability of the evidence E.\n",
    "The formula states that the probability of the hypothesis H given the evidence E is proportional to the likelihood of the evidence given the hypothesis, multiplied by the prior probability of the hypothesis, and divided by the probability of the evidence.\n",
    "\n",
    "In simpler terms, Bayes' theorem tells us how to update our belief about the probability of a hypothesis based on new evidence. It involves combining our prior knowledge about the hypothesis with the new evidence to arrive at a more accurate estimate of the probability of the hypothesis.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be31bb5-19d8-44f8-b9d1-52626e2fd4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How is Bayes' theorem used in practice?\n",
    "Ans-\n",
    "    Bayes' theorem is widely used in various fields, including statistics, machine learning, and artificial intelligence, to model uncertain events and make predictions based on incomplete or noisy data. Here are some practical applications of Bayes' theorem:\n",
    "\n",
    "1.Spam filtering: Bayes' theorem is used in email spam filtering to determine the probability that a message is spam or not. The likelihood is based on the occurrence of certain words or phrases in the email, while the prior probability is based on the frequency of spam emails in general.\n",
    "\n",
    "2.Medical diagnosis: Bayes' theorem is used in medical diagnosis to determine the probability that a patient has a certain disease based on their symptoms and test results. The likelihood is based on the accuracy of the medical tests, while the prior probability is based on the prevalence of the disease in the population.\n",
    "\n",
    "3.Stock market prediction: Bayes' theorem is used in financial analysis to predict the probability of a stock price going up or down based on various factors such as company performance, market trends, and news events. The likelihood is based on historical data and technical indicators, while the prior probability is based on market trends and other external factors.\n",
    "\n",
    "4.Image recognition: Bayes' theorem is used in image recognition to classify images into different categories based on their features. The likelihood is based on the occurrence of certain features in the image, while the prior probability is based on the frequency of images in each category.\n",
    "\n",
    "5.Natural language processing: Bayes' theorem is used in natural language processing to determine the probability of a sentence being grammatically correct or not. The likelihood is based on the occurrence of certain words or grammatical structures, while the prior probability is based on the frequency of correct sentences in the language.\n",
    "\n",
    "In each of these applications, Bayes' theorem is used to update our belief about the probability of an event or hypothesis based on new evidence. By combining our prior knowledge with the new evidence, we can arrive at a more accurate estimate of the probability and make better decisions or predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2a4a1d-93d3-4f2b-8863-323baff93b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the relationship between Bayes' theorem and conditional probability?\n",
    "Ans-\n",
    "   Bayes' theorem and conditional probability are closely related concepts in probability theory. Bayes' theorem can be derived from conditional probability, and it provides a way to update our beliefs about the probability of an event or hypothesis based on new evidence.\n",
    "\n",
    "Conditional probability is the probability of an event A occurring given that another event B has occurred, and it is denoted as P(A|B). It is defined as the ratio of the joint probability of A and B to the probability of B, i.e.:\n",
    "\n",
    "P(A|B) = P(A and B) / P(B)\n",
    "\n",
    "Bayes' theorem is a formula that describes the relationship between the conditional probability of an event and the probability of its causes. It states that the probability of an event A occurring given some evidence B is proportional to the conditional probability of B given A, multiplied by the prior probability of A, and divided by the evidence probability, i.e.:\n",
    "\n",
    "P(A|B) = P(B|A) * P(A) / P(B)\n",
    "\n",
    "where P(A) is the prior probability of A, P(B|A) is the conditional probability of B given A, and P(B) is the evidence probability.\n",
    "\n",
    "Bayes' theorem is a generalization of conditional probability that provides a way to update our beliefs about the probability of an event or hypothesis based on new evidence. It is widely used in various fields, including statistics, machine learning, and artificial intelligence, to model uncertain events and make predictions based on incomplete or noisy data.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f76c1d-bc4c-4198-a99b-6e5aa3983cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
    "Ans-\n",
    "    There are three main types of Naive Bayes classifiers: Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes. The choice of which type of classifier to use depends on the nature of the data and the specific problem being solved. Here are some general guidelines:\n",
    "\n",
    "1.Gaussian Naive Bayes: This classifier assumes that the feature values are normally distributed. It is suitable for continuous or real-valued features, and is often used in classification problems where the data is continuous, such as in image or speech recognition.\n",
    "\n",
    "2.Multinomial Naive Bayes: This classifier is suitable for discrete or count-based data, such as word counts in text classification. It assumes that the feature values are generated from a multinomial distribution.\n",
    "\n",
    "3.Bernoulli Naive Bayes: This classifier is similar to Multinomial Naive Bayes, but is used for binary or boolean features, where each feature can take on only two values (e.g. presence or absence of a feature). It assumes that the feature values are generated from a Bernoulli distribution.\n",
    "\n",
    "In general, Gaussian Naive Bayes is a good choice for continuous data, Multinomial Naive Bayes for discrete count data, and Bernoulli Naive Bayes for binary data. However, the choice of which type of classifier to use ultimately depends on the specific characteristics of the data and the problem being solved. It is often a good idea to try all three types of Naive Bayes classifiers and compare their performance on the given dataset to choose the most appropriate one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aaeb74-17ee-4ff5-86c4-d3a3bf51d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Assignment:\n",
    "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\n",
    "Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of\n",
    "each feature value for each class:\n",
    "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "A 3 3 4 4 3 3 3\n",
    "B 2 2 1 2 2 2 3\n",
    "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance\n",
    "to belong to?\n",
    "\n",
    "Ans-\n",
    "    To classify the new instance with features X1 = 3 and X2 = 4 using Naive Bayes, we need to calculate the posterior probability of each class given the feature values. We can use the Naive Bayes formula:\n",
    "\n",
    "P(A|X1=3,X2=4) = P(X1=3,X2=4|A) * P(A) / P(X1=3,X2=4)\n",
    "P(B|X1=3,X2=4) = P(X1=3,X2=4|B) * P(B) / P(X1=3,X2=4)\n",
    "\n",
    "Since the prior probabilities are equal, we can ignore them and focus on the likelihoods:\n",
    "\n",
    "P(X1=3,X2=4|A) = P(X1=3|A) * P(X2=4|A) = 4/13 * 3/13 = 0.090\n",
    "P(X1=3,X2=4|B) = P(X1=3|B) * P(X2=4|B) = 1/7 * 3/7 = 0.061\n",
    "\n",
    "To calculate the evidence probability P(X1=3,X2=4), we need to sum the product of each likelihood and prior probability for each class:\n",
    "\n",
    "P(X1=3,X2=4) = P(X1=3,X2=4|A) * P(A) + P(X1=3,X2=4|B) * P(B) = 0.090 * 0.5 + 0.061 * 0.5 = 0.0755\n",
    "\n",
    "Finally, we can calculate the posterior probabilities using Bayes' theorem:\n",
    "\n",
    "P(A|X1=3,X2=4) = 0.090 * 0.5 / 0.0755 = 0.597\n",
    "P(B|X1=3,X2=4) = 0.061 * 0.5 / 0.0755 = 0.403\n",
    "\n",
    "Therefore, Naive Bayes would predict the new instance to belong to class A, since it has the higher posterior probability of 0.597."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
