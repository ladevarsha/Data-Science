{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c61fac-f6dd-45d1-bc73-098a37f0b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "Ans-\n",
    "    The KNN (K-Nearest Neighbors) algorithm is a simple and popular supervised machine learning algorithm used for classification and regression tasks. It works by finding the K closest data points to a given test point in a feature space, and then assigns a label to that test point based on the majority class among its K nearest neighbors. In other words, the KNN algorithm assumes that points that are close to each other in the feature space are likely to belong to the same class or have similar values for a continuous target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8803c949-e54d-4a6f-af41-34a1587c84bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How do you choose the value of K in KNN?\n",
    "The choice of K in KNN algorithm is an important hyperparameter that can affect the accuracy of the model. There are several methods to select the value of K:\n",
    "\n",
    "Empirical observation: Try different values of K and evaluate the performance of the model on a validation set or through cross-validation. Choose the value of K that gives the best accuracy or the lowest error.\n",
    "Domain knowledge: Consider the nature of the problem and the characteristics of the data. For example, if the data has a lot of noise or outliers, a smaller value of K may be better. On the other hand, if the data is relatively smooth and continuous, a larger value of K may be more appropriate.\n",
    "\n",
    "Mathematical methods: Use statistical or optimization techniques to find the optimal value of K. For example, one approach is to use the elbow method, which plots the accuracy or error as a function of K and chooses the value of K at the elbow point where the improvement in performance starts to level off. Another approach is to use grid search or randomized search to explore a range of possible values for K and select the one that performs the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978532c3-2da9-485a-9cef-8efc37120481",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "The main difference between KNN classifier and KNN regressor is in the type of the target variable. KNN classifier is used for classification tasks where the target variable is categorical, while KNN regressor is used for regression tasks where the target variable is continuous.\n",
    "\n",
    "In KNN classification, the algorithm assigns a label to a test point based on the majority class among its K nearest neighbors. The output is a categorical variable representing the class of the test point.\n",
    "\n",
    "In KNN regression, the algorithm predicts the target variable of a test point based on the average or median value of its K nearest neighbors. The output is a continuous variable representing the predicted value of the test point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0bed45-6a89-4ca5-9f70-bcc3fa06a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How do you measure the performance of KNN?\n",
    "To measure the performance of KNN algorithm, we can use various evaluation metrics depending on the type of the problem:\n",
    "\n",
    "Classification: For classification problems, we can use metrics such as accuracy, precision, recall, F1 score, and ROC-AUC score. Accuracy measures the proportion of correctly classified instances, while precision and recall measure the trade-off between the number of true positives and false positives, and the number of true positives and false negatives, respectively. F1 score is a harmonic mean of precision and recall that takes into account both metrics. ROC-AUC score is a measure of the area under the receiver operating characteristic curve, which is a plot of the true positive rate against the false positive rate at different classification thresholds.\n",
    "\n",
    "Regression: For regression problems, we can use metrics such as mean squared error (MSE), mean absolute error (MAE), and R-squared (R2) score. MSE measures the average squared difference between the predicted and actual values, while MAE measures the average absolute difference. R2 score measures the proportion of variance in the target variable that is explained by the model.\n",
    "\n",
    "We can also use cross-validation to estimate the performance of the KNN algorithm on new unseen data, and to tune the value of K or other hyperparameters to improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67841e7-1635-4441-ab5e-8413a468d30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the curse of dimensionality in KNN?\n",
    "The curse of dimensionality in KNN refers to the problem that occurs when the number of features or dimensions in the feature space increases. As the number of dimensions increases, the amount of data required to cover the same space grows exponentially, and the density of the data points decreases. This makes it difficult for the KNN algorithm to find the K nearest neighbors, and can lead to overfitting or poor generalization of the model. To mitigate the curse of dimensionality in KNN, it is important to reduce the dimensionality of the data through feature selection, feature extraction, or dimensionality reduction techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d5007e-fbdd-4fd2-bfb7-31a689afd0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How do you handle missing values in KNN?\n",
    "Handling missing values in KNN can be challenging because the algorithm relies on the distance between data points to make predictions. Here are some common approaches to deal with missing values in KNN:\n",
    "\n",
    "Mean or median imputation: Replace missing values with the mean or median value of the corresponding feature across the available data.\n",
    "\n",
    "KNN imputation: Use the KNN algorithm to estimate the missing values based on the values of the K nearest neighbors. This approach can be effective when the missing values are sparse and the data has a low degree of correlation.\n",
    "\n",
    "Regression imputation: Use a regression model to predict the missing values based on the values of the other features. This approach can be more accurate than mean or median imputation, but requires a larger amount of data and may be computationally intensive.\n",
    "\n",
    "Multiple imputation: Generate multiple imputations of the missing values and combine them to obtain a more robust estimate of the data. This approach can be useful when there is a high degree of uncertainty about the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7cf51b-b2f9-4f1f-8f6b-4d4e0ae2f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "KNN classifier is typically better suited for problems where the target variable is categorical and the decision boundaries between classes are relatively simple. It can perform well in problems with a small number of features and a large number of data points, as it does not require the training of a model or the estimation of parameters. However, it can suffer from the curse of dimensionality and can be sensitive to the choice of the number of neighbors (K) and the distance metric.\n",
    "\n",
    "KNN regressor, on the other hand, is better suited for problems where the target variable is continuous and the relationship between the features and the target variable is more complex. It can capture non-linear relationships and can be used for both univariate and multivariate regression problems. However, it can be sensitive to the choice of K and the distance metric, and can suffer from overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb6e9c0-7cb0-45f4-af7f-f21897872f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "The strengths of the KNN algorithm for classification and regression tasks include its simplicity, flexibility, and ability to capture complex relationships between features and the target variable. KNN can also be used for both binary and multi-class classification, and for univariate and multivariate regression.\n",
    "\n",
    "The weaknesses of the KNN algorithm include its sensitivity to the choice of K and the distance metric, its computationally intensive nature, and its inability to handle missing data and imbalanced classes. These weaknesses can be addressed by using cross-validation to tune the hyperparameters, using distance metrics that are appropriate for the problem, using dimensionality reduction techniques to reduce the number of features, and using imputation methods to handle missing data.\n",
    "\n",
    "Another approach to improve the performance of KNN is to combine it with other algorithms such as ensemble methods, decision trees, or neural networks. For example, ensemble methods such as bagging or boosting can be used to reduce the variance or bias of KNN, while decision trees or neural networks can be used to capture non-linear relationships or interactions between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a624a42d-923a-4827-926c-3bb2c0fcca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "Euclidean distance and Manhattan distance are both distance metrics that can be used in KNN algorithm to calculate the distance between two data points. The main difference between them is the way they measure distance.\n",
    "\n",
    "Euclidean distance is the straight-line distance between two points in Euclidean space. It is calculated as the square root of the sum of the squared differences between the corresponding features. It is a common distance metric used in KNN, especially when the features are continuous.\n",
    "formula:\n",
    "d=((x1-x2)2+(y1-y2)2)1/2\n",
    "\n",
    "Manhattan distance, also known as taxicab distance, is the sum of the absolute differences between the corresponding features. It is called Manhattan distance because it is similar to the distance a taxi would travel on a rectangular grid of streets to reach its destination. It is useful when the features are discrete or categorical.\n",
    "formula:\n",
    "d= |x1-x2|+|y-y2|\n",
    "\n",
    "In general, Euclidean distance is more sensitive to outliers and can be affected by the scale of the data, while Manhattan distance is less sensitive to outliers and scale. Therefore, the choice of distance metric depends on the nature of the problem and the characteristics of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad0ff6e-f70b-4327-be7a-6e0dedcadeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. What is the role of feature scaling in KNN?\n",
    "Feature scaling is an important step in KNN because the algorithm relies on distance measures to make predictions. If the features have different scales or units, the features with larger values will dominate the distance calculations and can lead to biased predictions.\n",
    "\n",
    "Feature scaling is the process of transforming the features to a common scale or range. There are different methods of feature scaling, including standardization and normalization.\n",
    "\n",
    "Standardization scales the features to have zero mean and unit variance, which means that the values are centered around zero and have a standard deviation of one. This method is useful when the data follows a normal distribution or when the range of the values is large.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
