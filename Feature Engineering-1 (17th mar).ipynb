{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97afe59-3b9a-4095-970e-b69b3f336e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values.\n",
    "Ans-\n",
    "   Missing values in a dataset refer to the absence of a value for a particular variable or feature in an observation or record. Missing values can occur due to various reasons, such as errors in data collection, data loss during transmission, or human error in data entry.\n",
    "\n",
    "It is essential to handle missing values in a dataset because they can adversely affect the accuracy and reliability of data analysis and machine learning models. Missing values can lead to biased results, reduce the representativeness of the dataset, and even prevent certain data analysis techniques from being applied. Therefore, handling missing values is crucial to ensure the quality and reliability of data analysis and modeling.\n",
    "\n",
    "Some algorithms that are not affected by missing values include tree-based models such as decision trees, random forests, and gradient boosting, as well as algorithms based on rules such as association rule mining and rule-based classifiers. These algorithms can handle missing values in different ways, such as ignoring the missing values, treating them as a separate category, or imputing them with a substitute value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9379c-273a-45fc-9b3e-6c41850779c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2: List down techniques used to handle missing data. Give an example of each with python code.\n",
    "Ans-\n",
    "    There are several techniques used to handle missing data. Some of the commonly used techniques are:\n",
    "\n",
    "    1.Deletion: This involves removing the rows or columns that contain missing values. There are two types of deletion: listwise deletion and pairwise deletion.\n",
    "\n",
    "Example using Python:\n",
    "# creating a sample dataset with missing values\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = {'Name': ['John', 'Mike', 'Sarah', 'Kate', 'Tim'],\n",
    "        'Age': [25, 28, np.nan, 31, 27],\n",
    "        'Salary': [50000, 60000, 45000, np.nan, 55000]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# listwise deletion\n",
    "df1 = df.dropna() # remove rows with missing values\n",
    "print(df1)\n",
    "\n",
    "# pairwise deletion\n",
    "df2 = df.dropna(axis=1) # remove columns with missing values\n",
    "print(df2)\n",
    "\n",
    "     2.Imputation: This involves filling in the missing values with estimated values. There are several methods for imputation, such as mean imputation, median imputation, and mode imputation.\n",
    "\n",
    "Example using Python:\n",
    "# using mean imputation\n",
    "df['Age'] = df['Age'].fillna(df['Age'].mean()) # fill missing values with mean\n",
    "print(df)\n",
    "\n",
    "# using median imputation\n",
    "df['Salary'] = df['Salary'].fillna(df['Salary'].median()) # fill missing values with median\n",
    "print(df)\n",
    "\n",
    "# using mode imputation\n",
    "df['Name'] = df['Name'].fillna(df['Name'].mode()[0]) # fill missing values with mode\n",
    "print(df)\n",
    "\n",
    "    3.Prediction: This involves using machine learning algorithms to predict the missing values based on the other variables in the dataset.\n",
    "\n",
    "Example using Python:\n",
    "# using K-Nearest Neighbors imputation\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "df_imputed = imputer.fit_transform(df[['Age', 'Salary']])\n",
    "\n",
    "df['Age'] = df_imputed[:, 0]\n",
    "df['Salary'] = df_imputed[:, 1]\n",
    "\n",
    "print(df)\n",
    "\n",
    "    4.Interpolation: This involves filling in the missing values using a mathematical function that estimates the missing values based on the values of the other variables in the dataset.\n",
    "\n",
    "Example using Python:\n",
    "# using linear interpolation\n",
    "df['Age'] = df['Age'].interpolate() # fill missing values using linear interpolation\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee4d89-7051-4d8d-82db-3312d9b9eda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "Ans-\n",
    "   Imbalanced data refers to a dataset where the number of observations in one class is significantly higher or lower than the number of observations in the other classes. In machine learning, imbalanced data can lead to biased models, where the algorithm learns to predict the majority class, ignoring the minority class.\n",
    "\n",
    "If imbalanced data is not handled, the machine learning model may perform poorly, leading to incorrect predictions for the minority class. This is because the model is trained on a dataset that is biased towards the majority class, and it does not learn to recognize patterns in the minority class.\n",
    "\n",
    "For example, in a medical diagnosis problem where the positive cases (disease) are only 10% of the dataset, the model can predict all the negative cases, and still achieve an accuracy of 90%. In this case, the model may seem accurate, but it is not useful for predicting the positive cases, which are of greater importance.\n",
    "\n",
    "To avoid this, it is essential to handle imbalanced data and apply techniques such as undersampling, oversampling, and Synthetic Minority Over-sampling Technique (SMOTE) to balance the dataset and improve the performance of the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2864dec1-6a6e-480b-8398-f2e015f5efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required.\n",
    "Ans-\n",
    "     Up-sampling and down-sampling are techniques used to balance the class distribution in an imbalanced dataset.\n",
    "\n",
    "Down-sampling involves randomly removing samples from the majority class to match the number of samples in the minority class. This technique is used when the dataset is large, and the majority class has a much higher number of samples than the minority class. For example, in a dataset with 1000 samples, where 950 samples belong to the majority class and 50 samples belong to the minority class, down-sampling can be used to randomly remove 900 samples from the majority class, leaving 50 samples in each class.\n",
    "\n",
    "    Here's an example of down-sampling using the Python sklearn library:\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# assuming we have majority_class_samples and minority_class_samples as dataframes\n",
    "# Downsample majority class\n",
    "downsampled_majority = resample(majority_class_samples, replace=False, n_samples=len(minority_class_samples), random_state=42)\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "downsampled_df = pd.concat([downsampled_majority, minority_class_samples])\n",
    "Up-sampling, on the other hand, involves randomly replicating samples from the minority class to increase their number and match the number of samples in the majority class. This technique is used when the dataset is small, and the minority class has much fewer samples than the majority class. For example, in a dataset with 100 samples, where 10 samples belong to the minority class and 90 samples belong to the majority class, up-sampling can be used to randomly replicate the minority class samples to make a total of 90 samples in each class.\n",
    "\n",
    "    Here's an example of up-sampling using the Python sklearn library:\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# assuming we have majority_class_samples and minority_class_samples as dataframes\n",
    "# Upsample minority class\n",
    "upsampled_minority = resample(minority_class_samples, replace=True, n_samples=len(majority_class_samples), random_state=42)\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "upsampled_df = pd.concat([majority_class_samples, upsampled_minority])\n",
    "It is important to note that both up-sampling and down-sampling have their own advantages and disadvantages, and the choice between them should be based on the specific problem and dataset at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ddbe65-3802-4775-aba5-26ead7ec32b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5: What is data Augmentation? Explain SMOTE.\n",
    "Ans-\n",
    "    Data augmentation is a technique used to increase the size of a dataset by creating new examples by applying various transformations on the existing data. It is a commonly used technique in machine learning to deal with limited data availability and to improve the performance of models.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a data augmentation method used to deal with imbalanced datasets. It creates synthetic samples of the minority class by generating new examples based on the existing minority class samples. The basic idea of SMOTE is to interpolate between the feature vectors of the minority class samples to create new synthetic samples.\n",
    "\n",
    "The SMOTE algorithm works as follows:\n",
    "\n",
    "1.For each sample in the minority class, find its k nearest neighbors.\n",
    "2.Select one of the k neighbors randomly, and compute the difference between the feature vector of the sample and the feature vector of the selected neighbor.\n",
    "3.Multiply this difference by a random value between 0 and 1, and add the result to the feature vector of the minority class sample to create a new synthetic sample.\n",
    "4.Repeat steps 2 and 3 to create the desired number of new synthetic samples.\n",
    "\n",
    "SMOTE helps to balance the class distribution in the dataset and improves the performance of machine learning models, especially for classification problems. It is commonly used in applications such as fraud detection, medical diagnosis, and customer churn prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ae5769-8a2a-41e5-8a27-de34864948f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "Ans-\n",
    "   Outliers are data points that lie far away from the rest of the data in a dataset. Outliers can occur due to various reasons such as measurement errors, incorrect data entry, or natural variation in the data.\n",
    "\n",
    "It is essential to handle outliers because they can have a significant impact on statistical analyses and machine learning models. Outliers can lead to inaccurate estimates of summary statistics, biased results, and reduced model performance.\n",
    "\n",
    "There are several techniques to handle outliers, including:\n",
    "\n",
    "1.Removing outliers: In this approach, outliers are identified and removed from the dataset. However, this approach can lead to a loss of information and may not always be the best option.\n",
    "\n",
    "2.Winsorizing: This approach involves capping the extreme values in the dataset at a certain percentile value. This method can help reduce the influence of outliers without completely removing them.\n",
    "\n",
    "3.Transformation: This approach involves transforming the data to reduce the impact of outliers. For example, taking the log of the data can help normalize the distribution and reduce the impact of outliers.\n",
    "\n",
    "4.Robust statistical methods: These methods are designed to be less sensitive to outliers. For example, the median is a more robust measure of central tendency than the mean.\n",
    "\n",
    "5.Machine learning algorithms: Some machine learning algorithms, such as decision trees and random forests, are less sensitive to outliers than others.\n",
    "\n",
    "    Example of outlier removal with Python code:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# create a sample dataset with outliers\n",
    "data = pd.DataFrame({'A': np.random.normal(0, 1, 100),\n",
    "                     'B': np.random.normal(0, 1, 100),\n",
    "                     'C': np.random.normal(0, 1, 100)})\n",
    "data.loc[0, 'A'] = 10  # add outlier\n",
    "\n",
    "# identify and remove outliers\n",
    "q1 = data.quantile(0.25)\n",
    "q3 = data.quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "data_clean = data[~((data < (q1 - 1.5 * iqr)) | (data > (q3 + 1.5 * iqr))).any(axis=1)]\n",
    "\n",
    "print('Original data shape:', data.shape)\n",
    "print('Cleaned data shape:', data_clean.shape)\n",
    "\n",
    "   Example of Winsorizing with Python code:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# create a sample dataset with outliers\n",
    "data = pd.DataFrame({'A': np.random.normal(0, 1, 100),\n",
    "                     'B': np.random.normal(0, 1, 100),\n",
    "                     'C': np.random.normal(0, 1, 100)})\n",
    "data.loc[0, 'A'] = 10  # add outlier\n",
    "\n",
    "# Winsorize the extreme values\n",
    "p = 0.05  # percentiles to cap at\n",
    "data_winsorized = data.apply(lambda x: np.clip(x, x.quantile(p), x.quantile(1-p)))\n",
    "\n",
    "print('Original data:\\n', data.head())\n",
    "print('\\nWinsorized data:\\n', data_winsorized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72545fd4-c37f-4c7d-b00e-76a72fdf6f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "Ans-\n",
    "   There are several techniques that can be used to handle missing data in customer data analysis. Some of these techniques are:\n",
    "\n",
    "1.Deletion: This technique involves removing the rows or columns that contain missing values. This technique can be used when the missing values are very few in number and are randomly distributed. There are two types of deletion techniques:\n",
    "\n",
    "    a. Listwise deletion: In this technique, the entire row is deleted if it contains any missing value.\n",
    "\n",
    "    b. Pairwise deletion: In this technique, only the missing values are ignored for each calculation.\n",
    "\n",
    "2.Mean/median imputation: In this technique, the missing values are replaced with the mean or median value of the respective feature. This technique can be used when the missing values are few and the data is normally distributed.\n",
    "\n",
    "3.Mode imputation: In this technique, the missing values are replaced with the mode value of the respective feature. This technique can be used when the missing values are few and the data is categorical.\n",
    "\n",
    "4.Regression imputation: In this technique, a regression model is used to predict the missing values based on the values of other features. This technique can be used when the missing values are non-random and the data has a linear relationship.\n",
    "\n",
    "Example code:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Creating a sample dataset with missing values\n",
    "data = pd.DataFrame({'A': [1, 2, np.nan, 4, 5], 'B': [6, np.nan, 8, 9, 10]})\n",
    "\n",
    "# Mean imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data_mean = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "\n",
    "# Mode imputation\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "data_mode = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "\n",
    "# Regression imputation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "x_train = data.dropna().drop('A', axis=1)\n",
    "y_train = data.dropna()['A']\n",
    "x_test = data[data['A'].isna()].drop('A', axis=1)\n",
    "y_test = lin_reg.fit(x_train, y_train).predict(x_test)\n",
    "data_regression = data.copy()\n",
    "data_regression.loc[data['A'].isna(), 'A'] = y_test\n",
    "\n",
    "# Deletion\n",
    "data_deletion = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a178c1b-7374-4d53-a44a-b33af3858014",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?\n",
    "Ans-\n",
    "   There are various strategies that can be used to determine if the missing data is missing at random or if there is a pattern to the missing data. Here are some common techniques:\n",
    "\n",
    "1.Visualization: One approach is to create visualizations of the missing data. For example, you can use a heatmap to visualize the missing data across different variables. If the missing data is random, then the heatmap will show a random pattern of missing data across different variables. However, if there is a pattern to the missing data, then the heatmap will show clusters of missing data across certain variables.\n",
    "\n",
    "2.Statistical tests: Another approach is to use statistical tests to determine if the missing data is missing at random or not. One common test is the Little's MCAR (Missing Completely At Random) test. This test checks if the missing data is independent of both observed and unobserved data. If the test fails to reject the null hypothesis, then the missing data can be considered missing at random.\n",
    "\n",
    "3.Imputation: Another approach is to use imputation methods to fill in the missing data. By comparing the imputed values to the actual values, it is possible to determine if there is a pattern to the missing data. For example, if the imputed values are consistently higher or lower than the actual values, then there may be a systematic bias in the missing data.\n",
    "\n",
    "Overall, it is important to carefully examine the missing data to determine if it is missing at random or if there is a pattern to the missing data. This can help inform the choice of imputation method or other strategies for handling the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fe8848-163d-4ff1-8303-1bda90fbdd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "Ans-\n",
    "    When working with imbalanced datasets, it's important to use evaluation metrics that are sensitive to both the minority and majority classes. Some strategies for evaluating the performance of a machine learning model on an imbalanced dataset include:\n",
    "\n",
    "1.Confusion matrix: A confusion matrix provides information on the true positives, true negatives, false positives, and false negatives in a classification problem. It's a useful tool for understanding the performance of a model on imbalanced datasets.\n",
    "\n",
    "2.Precision, Recall, and F1-score: Precision is the ratio of true positives to the total number of positive predictions made by the model. Recall is the ratio of true positives to the total number of actual positive cases in the dataset. F1-score is the harmonic mean of precision and recall. These metrics are useful for evaluating the performance of a model on imbalanced datasets.\n",
    "\n",
    "3.ROC-AUC Curve: ROC-AUC (Receiver Operating Characteristic - Area Under the Curve) curve is another useful tool for evaluating the performance of a model on imbalanced datasets. It shows the trade-off between sensitivity (recall) and specificity (true negative rate) at different classification thresholds.\n",
    "\n",
    "4.Resampling Techniques: There are several resampling techniques that can be used to balance the class distribution in the dataset. For example, upsampling the minority class, downsampling the majority class, or using synthetic data generation techniques like SMOTE. These techniques can be used to balance the dataset before training the model.\n",
    "\n",
    "5.Cost-Sensitive Learning: Cost-sensitive learning is a technique that assigns different costs to different types of errors. For example, a false negative in a medical diagnosis task can be more costly than a false positive. By assigning different costs to different types of errors, the model can be optimized to minimize the overall cost of errors.\n",
    "\n",
    "6.Ensemble Learning: Ensemble methods can be used to combine multiple models to improve the overall performance on imbalanced datasets. For example, a combination of oversampled and undersampled models can be used to achieve better performance on imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a914e81-6544-49e6-805e-0ca7cc823285",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?\n",
    "Ans-\n",
    "    If the majority class is overrepresented in a dataset, it can be downsampled to balance the dataset. Here are some methods that can be employed to balance the dataset and down-sample the majority class:\n",
    "\n",
    "1.Random under-sampling: In this method, a random sample of the majority class is removed to balance the dataset. However, this method may result in the loss of useful information.\n",
    "\n",
    "   Here's an example of random under-sampling using Python:\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority_class = df[df.satisfaction == 'satisfied']\n",
    "minority_class = df[df.satisfaction == 'unsatisfied']\n",
    "\n",
    "# Downsample majority class\n",
    "majority_downsampled = resample(majority_class, replace=False, n_samples=len(minority_class), random_state=42)\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "downsampled = pd.concat([majority_downsampled, minority_class])\n",
    "\n",
    "# Check the class distribution\n",
    "downsampled.satisfaction.value_counts()\n",
    "\n",
    "2.Cluster-based under-sampling: In this method, the majority class is divided into clusters, and a representative sample is taken from each cluster.\n",
    "\n",
    "3.Tomek links: In this method, the samples of the majority and minority class are linked, and the samples that form a Tomek link are identified. The majority class samples that form Tomek links are removed.\n",
    "\n",
    "4.Edited nearest neighbors: In this method, the majority class samples that are misclassified by their k-nearest neighbors are removed.\n",
    "\n",
    "5.Synthetic Minority Over-sampling Technique (SMOTE): This method creates synthetic samples of the minority class by interpolating between the existing samples.\n",
    "\n",
    "   Here's an example of using SMOTE for up-sampling the minority class in Python:\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority_class = df[df.satisfaction == 'satisfied']\n",
    "minority_class = df[df.satisfaction == 'unsatisfied']\n",
    "\n",
    "# Upsample minority class using SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = sm.fit_resample(X, y)\n",
    "\n",
    "# Check the class distribution\n",
    "y_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b67ba2-82d0-48f2-ad55-def74febc8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?\n",
    "Ans-\n",
    "   If the dataset is unbalanced with a low percentage of occurrences of the event of interest, we can use the following methods to balance the dataset and up-sample the minority class:\n",
    "\n",
    "1.Random oversampling: This method involves randomly selecting samples from the minority class with replacement to create a balanced dataset.\n",
    "\n",
    "2.Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is a popular technique for oversampling the minority class. It involves creating synthetic examples of the minority class by interpolating between neighboring samples.\n",
    "\n",
    "3.ADASYN: ADASYN is another technique that is similar to SMOTE but focuses on generating synthetic samples in regions where the class imbalance is highest.\n",
    "\n",
    "    Here is an example of how to use the SMOTE technique in Python:\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "X_resampled, y_resampled = SMOTE().fit_resample(X, y)\n",
    "\n",
    "\n",
    "In this code snippet, X and y are the feature and target variables, respectively. The SMOTE() function is used to perform the oversampling operation, and the fit_resample() method is used to transform the data. The resulting X_resampled and y_resampled arrays contain the balanced dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
