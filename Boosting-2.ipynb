{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f0016c-eb20-4e60-9850-5d5098137036",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "Ans-\n",
    "    Gradient Boosting Regression is a machine learning algorithm used for regression tasks, which involves predicting a continuous numeric value as the output variable. It belongs to the family of ensemble methods and is based on the concept of boosting.\n",
    "\n",
    "The algorithm works by combining multiple weak prediction models, typically decision trees, to create a strong predictive model. The term \"gradient\" in Gradient Boosting refers to the technique used to minimize the loss function of the model. It iteratively trains the weak models in a sequential manner, where each subsequent model is trained to correct the mistakes made by the previous models.\n",
    "\n",
    "Here's a high-level overview of how Gradient Boosting Regression works:\n",
    "\n",
    "1.Initialize the model with a constant value, usually the mean of the target variable.\n",
    "2.Calculate the negative gradient of the loss function with respect to the current model's predictions. This gradient represents the direction and magnitude of the error.\n",
    "3.Fit a weak model, such as a decision tree, to the negative gradient, trying to minimize the loss function.\n",
    "4.Add the weak model to the ensemble, but with a small learning rate (shrinkage parameter) to prevent overfitting and control the contribution of each weak model.\n",
    "5.Update the predictions by adding the predictions of the newly added weak model, weighted by the learning rate.\n",
    "6.Repeat steps 2 to 5 for a specified number of iterations or until a certain stopping criterion is met.\n",
    "7.The final prediction is obtained by summing the predictions of all the weak models.\n",
    "\n",
    "By iteratively improving the model's predictions, Gradient Boosting Regression can create a powerful ensemble model capable of capturing complex relationships in the data. It is known for its high predictive accuracy and robustness against overfitting. However, it can be computationally expensive and requires careful tuning of hyperparameters to achieve optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a192a648-3a2c-46ac-bc7a-d836be2d7d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.\n",
    "Ans-\n",
    "\n",
    "    import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "## generate some random data for our regression problem\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "## split data into training and testing sets:\n",
    "split = int(len(X)*0.8)\n",
    "X_train, y_train = X[:split], y[:split]\n",
    "X_test, y_test = X[split:], y[split:]\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators, learning_rate):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # initialize the predictions\n",
    "        y_pred = np.zeros(len(y))\n",
    "        \n",
    "        # iterate over the number of estimators\n",
    "        for i in range(self.n_estimators):\n",
    "            # compute the gradient of the loss function\n",
    "            gradient = y - y_pred\n",
    "            \n",
    "            # fit a regression model to the gradient\n",
    "            model = DecisionTreeRegressor(max_depth=1)\n",
    "            model.fit(X, gradient)\n",
    "            \n",
    "            # add the model to the list of models\n",
    "            self.models.append(model)\n",
    "            \n",
    "            # update the predictions\n",
    "            y_pred += self.learning_rate * model.predict(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # initialize the predictions\n",
    "        y_pred = np.zeros(len(X))\n",
    "        \n",
    "        # iterate over the models and update the predictions\n",
    "        for model in self.models:\n",
    "            y_pred += self.learning_rate * model.predict(X)\n",
    "            \n",
    "        return y_pred\n",
    "    \n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gb.predict(X_test)\n",
    "\n",
    "y_pred\n",
    "\n",
    "array([-77.31321482,   4.16102018,   4.16102018, -63.05652357,\n",
    "        41.633057  ,   0.53797017, -22.76934693,   0.53797017,\n",
    "       -38.30103698,   0.53797017, -44.09854195,  63.40594491,\n",
    "         0.53797017, -22.76934693, -22.76934693,  62.99436584,\n",
    "       -63.05652357, -34.1208911 ,   0.53797017,  -8.95011837])\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared Score:\", r2)\n",
    "\n",
    "Mean Squared Error: 67.71525820905791\n",
    "R-squared Score: 0.9575588436634127\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bf1dbf-68b1-4ec1-baf8-ffc2a57cbb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters\n",
    "Ans-\n",
    "    import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=10, random_state=42)\n",
    "\n",
    "split = int(len(X)*0.8)\n",
    "X_train, y_train = X[:split], y[:split]\n",
    "X_test, y_test = X[split:], y[split:]\n",
    "\n",
    "# define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10,20,30,40,50],\n",
    "    'learning_rate': [0.01, 0.01, 1],\n",
    "    'max_depth': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# create an instance of the GradientBoostingRegressor\n",
    "gb = GradientBoostingRegressor()\n",
    "\n",
    "# create an instance of GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=gb, param_grid=param_grid, cv=5)\n",
    "\n",
    "# fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "GridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
    "             param_grid={'learning_rate': [0.01, 0.01, 1],\n",
    "                         'max_depth': [1, 2, 3],\n",
    "                         'n_estimators': [10, 20, 30, 40, 50]})\n",
    "\n",
    "# get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# create an instance of the GradientBoostingRegressor with the best hyperparameters\n",
    "gb = GradientBoostingRegressor(**best_params)\n",
    "\n",
    "# fit the model to the training data\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# create an instance of the GradientBoostingRegressor with the best hyperparameters\n",
    "gb = GradientBoostingRegressor(**best_params)\n",
    "\n",
    "# fit the model to the training data\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "GradientBoostingRegressor(learning_rate=1, max_depth=1, n_estimators=50)\n",
    "\n",
    "y_pred = gb.predict(X_test)\n",
    "y_pred\n",
    "\n",
    "array([ -30.95323312,  -79.39380921,   65.49935119, -181.05634672,\n",
    "       -100.29307511,  -20.4359374 , -164.63941687,   71.00112182,\n",
    "        -51.7829512 ,  -60.71661252, -138.12639865, -207.34729264,\n",
    "       -271.7140004 ,   25.22420287,  -85.82697331,    6.97388455,\n",
    "         86.74184225,  133.37624786,   71.14456023,  -18.10225891,\n",
    "         -2.56859274,   60.55991405,   91.2662676 ,  -82.13616056,\n",
    "        141.88245043,  -21.65343766,  -29.59597427,  176.95329294,\n",
    "        -85.99145152,  -17.51643494, -227.00742466, -144.11521404,\n",
    "        -78.63504533, -181.85805733, -239.7887483 ,   29.543985  ,\n",
    "       -110.97051887, -135.62188677,   79.86033693,  183.40055536,\n",
    "        -25.86953995,   36.17126283, -144.80393852,   19.8386141 ,\n",
    "         35.49998298, -315.11796203,   49.794025  ,  -72.63638294,\n",
    "         33.56144382,   55.19465723,  199.34296487,  127.97857209,\n",
    "         77.80963956,   83.15565606,   10.41301613,  -54.50619909,\n",
    "         44.63783562,  -87.98283721,   62.42099233,   29.8522689 ,\n",
    "       -139.42159365,   80.52159117,  -52.79338399,  356.82312278,\n",
    "        238.53968794,  164.86832705,    1.84540863,    5.37366267,\n",
    "         86.38653944, -206.86022965,   71.96635029,  -66.55051287,\n",
    "         83.97315976,  106.22755591,  -38.87812511,  -43.10253198,\n",
    "        -10.32238559,  -81.82483951, -198.45508673,  134.25238013,\n",
    "       -239.5791962 ,   14.97760004,  -55.25100934,  115.08994903,\n",
    "         13.1540767 ,   72.68167283,  -68.5296675 ,   90.68412441,\n",
    "        140.8869464 ,  144.40952525, -256.92398992,   -6.3744315 ,\n",
    "       -233.77915086,  173.63946751,   33.88447515,  -90.84745517,\n",
    "        124.31244426, -228.93719156,  -99.51175792,  -99.3427943 ,\n",
    "        -79.3494139 ,   25.3869847 ,   99.97191838,   36.1443649 ,\n",
    "        157.4691021 , -168.91487836,  -22.21675468,  219.63517665,\n",
    "       -208.96280545,   50.30469999,   18.71787851,  -18.92537321,\n",
    "         83.2392817 ,  -32.12854143, -116.36236928,   -2.04698514,\n",
    "        -22.180952  ,   -4.16308684, -123.42074301, -195.82560987,\n",
    "        -52.25820223,  -69.96011225, -127.70552828,  -37.61659479,\n",
    "       -134.92698567,   46.18943294,  -22.92747861, -102.4011998 ,\n",
    "         17.47334765,  -91.1132984 ,  -12.99731022,  148.53911441,\n",
    "         50.06874876,  -10.594364  ,  -19.46548149,  180.64161398,\n",
    "         62.76739062,  226.92489354,  175.09683168,  -41.04751801,\n",
    "        283.94483474,   79.12160882,  -39.2432803 ,  -11.42738906,\n",
    "        -52.6292072 ,    7.47549562, -128.12022957,  263.34958951,\n",
    "        233.89053854,   55.17631714,   76.73236555,  298.97457797,\n",
    "         29.68991132,  218.94725394,  -56.46214453,  168.99361441,\n",
    "        114.56263676, -140.65485961,  101.01363864,  -43.71704123,\n",
    "         38.71484609,  137.83713615,   46.28520494, -185.93140072,\n",
    "        107.21197638,  126.93897787, -294.76248471,  175.3172942 ,\n",
    "        -47.72876416, -226.37327686,  133.56786494,  -73.60743201,\n",
    "         76.39514766,  -85.05313725,   34.6008544 ,   73.91773989,\n",
    "         34.51007299,  -46.55912865,   14.67549864,  -97.09800107,\n",
    "        -45.50963162,   88.6166623 ,   51.67087278, -229.01743694,\n",
    "         23.20404538,   36.75471   ,  180.61408913,  -54.55405549,\n",
    "          8.76150565,  142.464305  ,   66.00475351,  247.22935104,\n",
    "        -38.04510005,  159.42729079,  292.65710584,   93.53107115,\n",
    "       -29.72737274,   31.42627031,   11.40385325,  -25.32078774])\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "Mean Squared Error: 1708.4062142829891\n",
    "R-squared: 0.893319454261174\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58da293f-f10d-4959-851c-2c14e5b7f599",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "Ans-\n",
    "    In Gradient Boosting, a weak learner is a model that performs slightly better than random guessing. Typically, decision trees with small depths are used as weak learners in Gradient Boosting.\n",
    "\n",
    "The reason for using weak learners is that they are simple and computationally efficient. They can be trained quickly and combined easily to form a powerful ensemble model. When combined, the weak learners can compensate for each other's weaknesses and produce a model that is much stronger than any individual weak learner.\n",
    "\n",
    "The role of the weak learner in Gradient Boosting is to make incremental improvements to the model's predictive accuracy. The weak learner is trained on the errors of the previous learner, with the goal of reducing those errors by a small amount. By repeatedly adding weak learners, the model can gradually improve its accuracy on the training data.\n",
    "\n",
    "Since the weak learners in Gradient Boosting are simple and have low variance, they are less likely to overfit the training data. This makes the model more robust and less prone to errors on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffbf95f-3b6f-4459-b307-506280a8229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "Ans-\n",
    "    The intuition behind the Gradient Boosting algorithm is to build a powerful ensemble model by iteratively adding weak learners (decision trees) to the model, each one correcting the errors made by the previous learners. The algorithm achieves this by minimizing the loss function of the model, which measures the difference between the predicted values and the actual values of the target variable.\n",
    "\n",
    "At each iteration, the algorithm trains a new decision tree on the errors of the previous iteration. The tree is trained to predict the residual errors of the previous model. The residuals represent the difference between the predicted and actual values of the target variable, and the new decision tree aims to reduce these residuals.\n",
    "\n",
    "The new decision tree is then added to the ensemble, and its predictions are combined with the predictions of the previous models to obtain a better overall prediction. This process is repeated for a predefined number of iterations, with each iteration producing a new decision tree that further improves the overall model.\n",
    "\n",
    "The key idea behind Gradient Boosting is that each subsequent tree tries to correct the mistakes made by the previous tree, and the algorithm adjusts the weights of the samples in each iteration to focus on the samples that were not correctly predicted by the previous trees. By doing this, the algorithm can gradually improve the accuracy of the model and achieve better predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c4f38-7949-4f1e-98bd-2de7c65a435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "Ans-\n",
    "    The Gradient Boosting algorithm builds an ensemble of weak learners by iteratively adding them to the model. The steps involved in the process are as follows:\n",
    "\n",
    "1.Train the initial weak learner: The first weak learner is trained on the training data to predict the target variable. This weak learner can be any model that performs slightly better than random guessing, but decision trees with small depths are commonly used.\n",
    "\n",
    "2.Compute the residuals: The predicted values of the first weak learner are subtracted from the actual values of the target variable to compute the residuals. These residuals represent the difference between the predicted and actual values and are used as the target variable for the next weak learner.\n",
    "\n",
    "3.Train subsequent weak learners: The subsequent weak learners are trained on the residuals of the previous weak learner. The goal is to minimize the residuals of the previous learner by fitting a new weak learner.\n",
    "\n",
    "4.Add the weak learner to the ensemble: The new weak learner is added to the ensemble of models, and its predictions are combined with the predictions of the previous models to obtain a better overall prediction.\n",
    "\n",
    "5.Repeat steps 2-4 until a stopping criterion is met: The process of adding weak learners is repeated until the ensemble of models reaches a predefined number of models or until the model's performance on the validation data stops improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27b0834-7b01-4e37-bbff-8260200a5689",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners by iteratively adding them to the model. The steps involved in the process are as follows:\n",
    "\n",
    "1.Train the initial weak learner: The first weak learner is trained on the training data to predict the target variable. This weak learner can be any model that performs slightly better than random guessing, but decision trees with small depths are commonly used.\n",
    "\n",
    "2.Compute the residuals: The predicted values of the first weak learner are subtracted from the actual values of the target variable to compute the residuals. These residuals represent the difference between the predicted and actual values and are used as the target variable for the next weak learner.\n",
    "\n",
    "3.Train subsequent weak learners: The subsequent weak learners are trained on the residuals of the previous weak learner. The goal is to minimize the residuals of the previous learner by fitting a new weak learner.\n",
    "\n",
    "4.Add the weak learner to the ensemble: The new weak learner is added to the ensemble of models, and its predictions are combined with the predictions of the previous models to obtain a better overall prediction.\n",
    "\n",
    "5.Repeat steps 2-4 until a stopping criterion is met: The process of adding weak learners is repeated until the ensemble of models reaches a predefined number of models or until the model's performance on the validation data stops improving."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
